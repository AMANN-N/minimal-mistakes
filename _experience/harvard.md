---
layout: portfolio-single
title: "Intern at Kreiman Lab @ Harvard Medical School"
excerpt: "Investigated humanâ€“machine linguistic imitation and Turing Test evaluation through controlled behavioral experiments and LLM benchmarking."
icon: "fas fa-brain"
image: "/assets/img/portfolio/kreiman.png"
weight: 4
github: ""
---

<div class="notice--info" style="display: flex; flex-wrap: wrap; gap: 20px; align-items: center; background: #f8f9fa; border-left: 4px solid #007bff; padding: 1em;">
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Role</span><br>
    <strong>Research Intern</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Company</span><br>
    <strong>Kreiman Lab (Harvard)</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Dates</span><br>
    <strong>Nov 2022 - June 2023</strong>
  </div>
</div>

## Overview
During my research internship at **Kreiman Lab â€” Harvard Medical School**, I led development and evaluation efforts for experimental AI systems designed to compare **human responses vs GPT-based LLMs** in linguistic reasoning tasks. I built a controlled **human experiment platform using JsPsych**, collected behavioral data, and analyzed LLM performance to support an academic publication.

**Publication:** [Turing Test Revisited: Exploring Humanâ€“Machine Imitation in Large Language Models](https://arxiv.org/abs/2211.13087)

---

## ðŸ” Research Problem
Evaluating â€œhuman-like intelligenceâ€ in AI requires more than benchmark scoresâ€”**we need experiments where humans interact and are compared directly against AI responses**, similar to the Turing Test, but measurable and repeatable.

---

## ðŸ›  Approach & Solution
- Built a **JsPsych-based experimental web platform** enabling structured interactions between participants and LLMs.
- Collected response samples from both humans and models to measure linguistic similarity, reasoning depth, and deception capability.
- Conducted **comparative benchmarking across multiple GPT-based models** using controlled Turing-Test-style evaluation.
- Automated experiment management and dataset processing to generate reproducible analysis reports.

---

## ðŸ“ˆ Key Contributions & Outcomes
| Contribution | Impact |
|-------------|--------|
| Built experiment system for large-scale human data collection | Enabled repeatable controlled testing |
| Designed behavioral scoring & model comparison pipeline | Reduced manual evaluation effort by **80%** |
| Benchmarked multiple LLMs under Turing-Test-style settings | Insights contributed to research publication |
| Supported interface design, data organization, and analysis | Improved research delivery speed and clarity |

---

## ðŸ“Š Experiment Workflow

![Experiment Diagram Placeholder](/assets/img/diagrams/turing_test_experiment_flow.png)
*Participant â†’ Web Interface â†’ GPT Model â†’ Response Capture â†’ Scoring â†’ Analysis & Comparison*

---

## ðŸ–¥ Screenshots / UI
| Experiment Interface | Human vs AI Evaluation Results |
|----------------------|-------------------------------|
| ![UI](/assets/img/screenshots/jspsych_ui.png) | ![Results](/assets/img/screenshots/experiment_data.png) |

---

## ðŸ§° Tech Stack
**Python**, PyTorch, HuggingFace Transformers, GPT models  
**JsPsych**, JavaScript, HTML/CSS  
Pandas, NumPy, Statistical Evaluation Tooling  

---

## â­ Featured Publication
> *Turing Test Revisited: Exploring Humanâ€“Machine Imitation in LLMs*  
> Published through Harvard Medical School research collaboration  
ðŸ”— https://arxiv.org/abs/2211.13087  

---

## ðŸ“¥ Demo & Access
ðŸ”’ Internal research system â€” demo and code accessible upon request

---
ðŸ“Ž Tools: GPT Models Â· JsPsych Â· PyTorch Â· Transformers Â· NumPy Â· Pandas  
ðŸŽ¯ Focus: Cognitive AI Â· Benchmarking Â· Behavioral Experimentation
---
