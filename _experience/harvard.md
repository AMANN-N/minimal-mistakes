---
layout: portfolio-single
title: "Research Intern â€” Kreiman Lab, Harvard Medical School"
excerpt: "Investigated humanâ€“machine linguistic imitation and Turing Test evaluation through controlled behavioral experiments and LLM benchmarking."
icon: "fas fa-brain"
image: "/assets/img/portfolio/kreiman-cover.png"
github: ""
---

> ðŸ§  **Objective:** Advance cognitive-AI research by studying how closely Large Language Models can approximate human communication and reasoning, using structured Turing Testâ€“style experiments.

---

## Overview
During my research internship at **Kreiman Lab â€” Harvard Medical School (Boston, MA | Remote)**, I led development and evaluation efforts for experimental AI systems designed to compare **human responses vs GPT-based LLMs** in linguistic reasoning tasks.  
I built a controlled **human experiment platform using JsPsych**, collected behavioral data, and analyzed LLM performance to support an academic publication.

ðŸ“… **Nov 2022 â€“ June 2023**  
ðŸ“„ **Publication:** *Turing Test Revisited: Exploring Humanâ€“Machine Imitation in Large Language Models*  
ðŸ”— [View paper on arXiv](https://arxiv.org/abs/2211.13087)

---

## ðŸ” Research Problem
Evaluating â€œhuman-like intelligenceâ€ in AI requires more than benchmark scoresâ€”**we need experiments where humans interact and are compared directly against AI responses**, similar to the Turing Test, but measurable and repeatable.

---

## ðŸ›  Approach & Solution
- Built a **JsPsych-based experimental web platform** enabling structured interactions between participants and LLMs.
- Collected response samples from both humans and models to measure linguistic similarity, reasoning depth, and deception capability.
- Conducted **comparative benchmarking across multiple GPT-based models** using controlled Turing-Test-style evaluation.
- Automated experiment management and dataset processing to generate reproducible analysis reports.

---

## ðŸ“ˆ Key Contributions & Outcomes
| Contribution | Impact |
|-------------|--------|
| Built experiment system for large-scale human data collection | Enabled repeatable controlled testing |
| Designed behavioral scoring & model comparison pipeline | Reduced manual evaluation effort by **80%** |
| Benchmarked multiple LLMs under Turing-Test-style settings | Insights contributed to research publication |
| Supported interface design, data organization, and analysis | Improved research delivery speed and clarity |

---

## ðŸ“Š Experiment Workflow

![Experiment Diagram Placeholder](/assets/img/diagrams/turing_test_experiment_flow.png)
*Participant â†’ Web Interface â†’ GPT Model â†’ Response Capture â†’ Scoring â†’ Analysis & Comparison*

---

## ðŸ–¥ Screenshots / UI
| Experiment Interface | Human vs AI Evaluation Results |
|----------------------|-------------------------------|
| ![UI](/assets/img/screenshots/jspsych_ui.png) | ![Results](/assets/img/screenshots/experiment_data.png) |

---

## ðŸ§° Tech Stack
**Python**, PyTorch, HuggingFace Transformers, GPT models  
**JsPsych**, JavaScript, HTML/CSS  
Pandas, NumPy, Statistical Evaluation Tooling  

---

## â­ Featured Publication
> *Turing Test Revisited: Exploring Humanâ€“Machine Imitation in LLMs*  
> Published through Harvard Medical School research collaboration  
ðŸ”— https://arxiv.org/abs/2211.13087  

---

## ðŸ“¥ Demo & Access
ðŸ”’ Internal research system â€” demo and code accessible upon request

---
ðŸ“Ž Tools: GPT Models Â· JsPsych Â· PyTorch Â· Transformers Â· NumPy Â· Pandas  
ðŸŽ¯ Focus: Cognitive AI Â· Benchmarking Â· Behavioral Experimentation
---
