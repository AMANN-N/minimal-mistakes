---
layout: portfolio-single
title: "GPT - Generative Pre-trained Transformer"
excerpt: "PyTorch implementation of OpenAI's original GPT model for generative language modeling."
icon: "fas fa-robot"
image: "/assets/img/implementations/gpt-cover.png"
github: "https://github.com/AMANN-N/ml-playground"
category: "advanced"
---

**Type:** Model Implementation  
**Framework:** PyTorch  
**Paper:** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## Overview
Implementation of OpenAI's GPT (Generative Pre-trained Transformer) from scratch in PyTorch, demonstrating the power of unsupervised pre-training for language understanding.

## Key Components
- Transformer decoder architecture
- Masked self-attention
- Pre-training and fine-tuning pipeline
- Byte-pair encoding (BPE) tokenization
- Generative language modeling

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Transfer learning, Unsupervised pre-training, Autoregressive modeling
- **Applications:** Text generation, language understanding, few-shot learning

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground)
