---
layout: portfolio-single
title: "GPT-2 - 124M Parameter Model"
excerpt: "PyTorch implementation of GPT-2 with 124 million parameters for advanced text generation and language modeling."
icon: "fas fa-brain"
image: "/assets/img/implementations/gpt2-cover.png"
github: "https://github.com/AMANN-N/ml-playground"
category: "advanced"
---

<div class="notice--info" style="display: flex; flex-wrap: wrap; gap: 20px; align-items: center; background: #e8f4f8; border-left: 4px solid #17a2b8; padding: 1em;">
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Type</span><br>
    <strong>Large-Scale Model Implementation</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Framework</span><br>
    <strong>PyTorch</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Paper</span><br>
    <strong><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></strong>
  </div>
</div>

## Overview
Implementation of GPT-2 (124 million parameter version) from scratch in PyTorch, showcasing the capabilities of large-scale language models for zero-shot task transfer.

## Key Components
- Scaled transformer architecture (12 layers, 768 hidden dimensions)
- 124 million trainable parameters
- Layer normalization improvements
- Advanced tokenization (BPE)
- Multi-task learning capabilities

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Large-scale language modeling, Zero-shot learning, Transfer learning
- **Applications:** Text generation, summarization, translation, question answering

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground)
