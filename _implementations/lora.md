---
layout: portfolio-single
title: "LoRA - Low-Rank Adaptation"
excerpt: "Implementation of LoRA (Low-Rank Adaptation) for efficient fine-tuning of large language models."
icon: "fas fa-compress-alt"
image: "/assets/img/implementations/lora-cover.png"
github: "https://github.com/AMANN-N/ml-playground/tree/main/LoRA"
---

**Type:** Parameter-Efficient Fine-Tuning  
**Framework:** PyTorch  
**Paper:** [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

## Overview
Implementation of LoRA (Low-Rank Adaptation) technique for efficiently fine-tuning large pre-trained models by injecting trainable low-rank matrices.

## Key Components
- Low-rank decomposition
- Adapter layers
- Freezing pre-trained weights
- Efficient parameter updates
- Merge and inference optimization

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Low-rank approximation, Transfer learning, Parameter efficiency
- **Applications:** LLM fine-tuning, Domain adaptation, Task-specific customization

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground/tree/main/LoRA)
