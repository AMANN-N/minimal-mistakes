---
layout: portfolio-single
title: "Word2Vec - Word Embeddings"
excerpt: "PyTorch implementation of Word2Vec for learning distributed representations of words from large text corpora."
icon: "fas fa-language"
image: "/assets/img/implementations/word2vec-cover.png"
github: "https://github.com/AMANN-N/ml-playground"
category: "deep-learning"
---

<div class="notice--info" style="display: flex; flex-wrap: wrap; gap: 20px; align-items: center; background: #e8f4f8; border-left: 4px solid #17a2b8; padding: 1em;">
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Type</span><br>
    <strong>Paper Implementation</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Framework</span><br>
    <strong>PyTorch</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Paper</span><br>
    <strong><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></strong>
  </div>
</div>

## Overview
Implementation of Word2Vec algorithm from scratch in PyTorch, including both Skip-gram and CBOW architectures for learning word embeddings.

## Key Components
- Skip-gram model
- Continuous Bag of Words (CBOW)
- Negative sampling
- Hierarchical softmax
- Word vector representations

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Word embeddings, Distributional semantics, Neural language models
- **Applications:** NLP, semantic similarity, word analogies

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground/tree/main/Word2Vec)
