---
layout: portfolio-single
title: "Word2Vec - Word Embeddings"
excerpt: "PyTorch implementation of Word2Vec for learning distributed representations of words from large text corpora."
icon: "fas fa-language"
image: "/assets/img/implementations/word2vec-cover.png"
github: "https://github.com/AMANN-N/ml-playground"
---

**Type:** Paper Implementation  
**Framework:** PyTorch  
**Paper:** [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

## Overview
Implementation of Word2Vec algorithm from scratch in PyTorch, including both Skip-gram and CBOW architectures for learning word embeddings.

## Key Components
- Skip-gram model
- Continuous Bag of Words (CBOW)
- Negative sampling
- Hierarchical softmax
- Word vector representations

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Word embeddings, Distributional semantics, Neural language models
- **Applications:** NLP, semantic similarity, word analogies

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground)
