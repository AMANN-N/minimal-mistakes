---
layout: portfolio-single
title: "RNN - Recurrent Neural Network"
excerpt: "Implementation of Recurrent Neural Networks from scratch for sequence modeling and time-series prediction."
icon: "fas fa-sync-alt"
image: "/assets/img/implementations/rnn-cover.png"
github: "https://github.com/AMANN-N/ml-playground/tree/main/RNN"
category: "deep-learning"
---

<div class="notice--info" style="display: flex; flex-wrap: wrap; gap: 20px; align-items: center; background: #e8f4f8; border-left: 4px solid #17a2b8; padding: 1em;">
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Type</span><br>
    <strong>Sequential Deep Learning</strong>
  </div>
  <div>
    <span style="color: #6c757d; font-size: 0.9em; text-transform: uppercase; letter-spacing: 1px;">Framework</span><br>
    <strong>PyTorch/NumPy</strong>
  </div>
</div>  

## Overview
Implementation of Recurrent Neural Networks (RNN) from scratch, including vanilla RNN, LSTM, and GRU variants for sequence modeling.

## Key Components
- Vanilla RNN cells
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)
- Backpropagation through time (BPTT)
- Sequence-to-sequence architecture

## Technologies Used
- **Framework:** PyTorch, NumPy
- **Concepts:** Sequential modeling, Hidden states, Gradient flow
- **Applications:** Language modeling, Time-series prediction, Sequence generation

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground/tree/main/RNN)
