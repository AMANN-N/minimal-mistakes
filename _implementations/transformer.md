---
layout: portfolio-single
title: "Transformer - Attention Is All You Need"
excerpt: "Implementation of the groundbreaking Transformer architecture from the 'Attention Is All You Need' paper using PyTorch."
icon: "fas fa-brain"
image: "/assets/img/implementations/transformer-cover.png"
github: "https://github.com/AMANN-N/ml-playground"
---

**Type:** Paper Implementation  
**Framework:** PyTorch  
**Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Overview
Complete implementation of the Transformer architecture from scratch in PyTorch, following the seminal "Attention Is All You Need" paper by Vaswani et al.

## Key Components
- Multi-head self-attention mechanism
- Positional encoding
- Encoder-decoder architecture
- Feed-forward networks
- Layer normalization and residual connections

## Technologies Used
- **Framework:** PyTorch
- **Concepts:** Self-attention, Multi-head attention, Positional encoding
- **Applications:** Machine translation, sequence-to-sequence tasks

## GitHub Repository
ðŸ”— [View on GitHub](https://github.com/AMANN-N/ml-playground)
